{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ce736d6",
   "metadata": {},
   "source": [
    "# ⚡ Training Workflow (1–14)\n",
    "이 구간은 빠르게 데이터 일부로 end-to-end 학습과 검증을 수행하는 스모크 테스트 플로우입니다. 아래 순서대로 각 셀을 실행하세요."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b69e56",
   "metadata": {},
   "source": [
    "# 1. Import required libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "86b9e541",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SmokeTest | PyTorch: 2.8.0+cu128 | CUDA? True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='TRUE'\n",
    "import sys, math, json, random, contextlib\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torch_geometric.datasets import AirfRANS\n",
    "from torch_geometric.data import Data, Batch\n",
    "from matplotlib.tri import Triangulation\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "from torch_geometric.data import Data\n",
    "from navier_stokes_physics_loss import NavierStokesPhysicsLoss\n",
    "from airfrans_utils import prepare_airfrans_graph_for_physics, estimate_node_area, build_bc_masks_airfrans\n",
    "import contextlib\n",
    "import wandb  \n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "\n",
    "def get_lr(optim):\n",
    "    return optim.param_groups[0].get('lr', None)\n",
    "\n",
    "\n",
    "def set_seed(seed: int = 42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "set_seed(42)\n",
    "print('SmokeTest | PyTorch:', torch.__version__, '| CUDA?', torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73319def-4d15-4886-b4b2-9bf2b9d666d2",
   "metadata": {},
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25c0e0db-f818-427f-9a37-c337838180e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smoke config: {'seed': 42, 'task': 'scarce', 'root': 'Dataset', 'limit_train': 180, 'limit_val': 20, 'batch_size': 19, 'epochs': 100, 'hidden': 128, 'layers': 7, 'lr': 0.0004, 'weight_decay': 0.01, 'betas': (0.9, 0.95), 'eps': 1e-08, 'amp': False, 'lr_scheduler': 'cosine', 'cosine_T_max': 80, 'cosine_eta_min': 1e-06, 'wr_T_0': 10, 'wr_T_mult': 1, 'wr_eta_min': 1e-06, 'rop_factor': 0.5, 'rop_patience': 5, 'rop_min_lr': 1e-06, 'ramp_start_epoch': 30, 'ramp_epochs': 80, 'ramp_mode': 'linear', 'data_loss_weight': 1.0, 'continuity_loss_weight': 0.05, 'continuity_target_weight': 0.1, 'momentum_loss_weight': 0.05, 'momentum_target_weight': 0.1, 'bc_loss_weight': 0.05, 'chord_length': 1.0, 'nu_molecular': 1.5e-05, 'dynamic_uref_from_data': True, 'dynamic_re_from_data': True, 'uinf_from': 'inlet', 'use_huber_for_physics': True, 'huber_delta': 0.05, 'use_perimeter_norm_for_div': True, 'div_area_floor_factor': 0.25, 'div_min_degree': 2, 'physics_debug': False, 'physics_debug_level': 1, 'physics_debug_every': 50, 'use_global_tokens': True, 'num_global_tokens': 4, 'attention_heads': 4, 'attention_layers': 7, 'attention_dropout': 0.0, 'use_cross_attention': True, 'global_pooling_type': 'attention', 'positional_encoding': True, 'pos_encoding_max_len': 50000, 'use_residual_attention': True, 'attention_normalization': 'layer', 'temperature_scaling': True, 'attention_bias': False, 'use_wandb_artifacts': False, 'artifact_save_best_only': True, 'artifact_save_interval': 50, 'ckpt_dir': 'checkpoints', 'ckpt_interval': 5, 'wandb_project': 'storm', 'wandb_mode': 'online', 'log_every_n_steps': -1, 'log_epoch_only': True}\n"
     ]
    }
   ],
   "source": [
    "# 2) Configuration (minimal for smoke)\n",
    "from dataclasses import dataclass, asdict\n",
    "\n",
    "@dataclass\n",
    "class SmokeCfg:\n",
    "    seed: int = 42\n",
    "    task: str = 'scarce'\n",
    "    root: str = 'Dataset'\n",
    "    # subsample graph count for smoke\n",
    "    limit_train: int = 180\n",
    "    limit_val: int = 20\n",
    "\n",
    "    # training\n",
    "    batch_size: int = 19\n",
    "    epochs: int = 100\n",
    "    hidden: int = 128\n",
    "    layers: int = 7\n",
    "    lr: float = 4e-4\n",
    "    weight_decay: float = 1e-2  # typical AdamW wd\n",
    "    betas: tuple[float, float] = (0.9, 0.95)\n",
    "    eps: float = 1e-8\n",
    "    amp: bool = False\n",
    "\n",
    "    # lr scheduler: 'cosine', 'cosine_warm_restarts', 'reduce_on_plateau', or None\n",
    "    lr_scheduler: str = 'cosine'\n",
    "    # cosine params\n",
    "    cosine_T_max: int = 80  # epochs\n",
    "    cosine_eta_min: float = 1e-6\n",
    "    # warm restarts params\n",
    "    wr_T_0: int = 10\n",
    "    wr_T_mult: int = 1\n",
    "    wr_eta_min: float = 1e-6\n",
    "    # reduce on plateau params\n",
    "    rop_factor: float = 0.5\n",
    "    rop_patience: int = 5\n",
    "    rop_min_lr: float = 1e-6\n",
    "\n",
    "    # Physics-Informed Loss Configuration\n",
    "    # =====================================\n",
    "    # Curriculum learning schedule\n",
    "    ramp_start_epoch: int = 30              # Epoch to start ramping physics losses\n",
    "    ramp_epochs: int = 80                   # Number of epochs to ramp up\n",
    "    ramp_mode: str = 'linear'               # 'linear' or 'cosine'\n",
    "    \n",
    "    # MSE/Data loss\n",
    "    data_loss_weight: float = 1.0           # Weight for MSE loss (constant)\n",
    "    \n",
    "    # Continuity equation loss\n",
    "    continuity_loss_weight: float = 0.05    # Initial continuity weight\n",
    "    continuity_target_weight: float = 0.10  # Target continuity weight after ramp\n",
    "    \n",
    "    # Momentum equation loss  \n",
    "    momentum_loss_weight: float = 0.05      # Initial momentum weight\n",
    "    momentum_target_weight: float = 0.10    # Target momentum weight after ramp\n",
    "    \n",
    "    # Boundary condition loss\n",
    "    bc_loss_weight: float = 0.05            # Weight for boundary condition loss\n",
    "    \n",
    "    # Physics parameters\n",
    "    chord_length: float = 1.0               # Airfoil chord length\n",
    "    nu_molecular: float = 1.5e-5            # Molecular viscosity\n",
    "    dynamic_uref_from_data: bool = True     # Compute reference velocity from data\n",
    "    dynamic_re_from_data: bool = True       # Compute Reynolds number from data\n",
    "    uinf_from: str = 'inlet'                # 'inlet', 'farfield', or 'robust'\n",
    "    \n",
    "    # Stability & outlier control\n",
    "    use_huber_for_physics: bool = True      # Use Huber loss for physics terms\n",
    "    huber_delta: float = 0.05               # Huber loss delta parameter\n",
    "    use_perimeter_norm_for_div: bool = True # Normalize divergence by perimeter\n",
    "    div_area_floor_factor: float = 0.25     # Area floor factor for stability\n",
    "    div_min_degree: int = 2                 # Minimum node degree for physics loss\n",
    "    \n",
    "    # Debug & monitoring\n",
    "    physics_debug: bool = False              # Enable physics loss debugging\n",
    "    physics_debug_level: int = 1            # Debug verbosity (1=summary, 2=detailed)\n",
    "    physics_debug_every: int = 50           # Log debug info every N steps\n",
    "\n",
    "    # Global Context & Attention Configuration\n",
    "    use_global_tokens: bool = True           # Enable/disable global tokens\n",
    "    num_global_tokens: int = 4               # Number of global tokens\n",
    "    attention_heads: int = 4                 # Multi-head attention heads\n",
    "    attention_layers: int = 7               # Number of transformer layers\n",
    "    attention_dropout: float = 0.0           # Attention dropout rate\n",
    "    use_cross_attention: bool = True         # Cross-attention between local and global\n",
    "    global_pooling_type: str = 'attention'   # 'mean', 'max', 'attention', 'set2set'\n",
    "    positional_encoding: bool = True         # Use positional encoding\n",
    "    pos_encoding_max_len: int = 50000        # Max sequence length for positional encoding\n",
    "    # Advanced attention options\n",
    "    use_residual_attention: bool = True      # Residual connections in attention\n",
    "    attention_normalization: str = 'layer'   # 'layer', 'batch', 'rms'\n",
    "    temperature_scaling: bool = True         # Temperature scaling for attention\n",
    "    attention_bias: bool = False             # Use bias in attention projections\n",
    "\n",
    "    # W&B Artifact 관리\n",
    "    use_wandb_artifacts: bool = False        # W&B artifact 사용 여부\n",
    "    artifact_save_best_only: bool = True     # best 모델만 업로드\n",
    "    artifact_save_interval: int = 50         # periodic 저장 간격 (epochs)\n",
    "    \n",
    "    # Checkpoint 관리\n",
    "    ckpt_dir: str = \"checkpoints\"           # 로컬 체크포인트 디렉토리\n",
    "    ckpt_interval: int = 5                  # 로컬 체크포인트 저장 간격\n",
    "    \n",
    "    # W&B 설정\n",
    "    wandb_project: str = \"storm\"\n",
    "    wandb_mode: str = \"online\"              # \"online\", \"offline\", \"disabled\"\n",
    "    log_every_n_steps: int = -1             # 로깅 빈도\n",
    "    log_epoch_only: bool = True             # Epoch 로깅만 사용\n",
    "\n",
    "scfg = SmokeCfg()\n",
    "set_seed(scfg.seed)\n",
    "print('Smoke config:', asdict(scfg))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded62e7d",
   "metadata": {},
   "source": [
    "# 3. Load dataset indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "37147c6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded subset indices: 200 train | 0 val/test\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.transforms import BaseTransform\n",
    "\n",
    "class _PreparePhysics(BaseTransform):\n",
    "    def __call__(self, data):\n",
    "        # edge_attr_dxdy가 이미 있을 경우 build_edge_attr_dxdy는 생략되고 나머지만 수행\n",
    "        return prepare_airfrans_graph_for_physics(data, verbose=False)\n",
    "\n",
    "# 3) Load dataset indices (train/val split)\n",
    "assert os.path.isdir(scfg.root), f\"Dataset folder not found: {scfg.root}\"\n",
    "try:\n",
    "    ds_train = AirfRANS(root=scfg.root, train=True, task=scfg.task, transform=_PreparePhysics())\n",
    "    ds_test  = AirfRANS(root=scfg.root, train=False, task=scfg.task, transform=_PreparePhysics())\n",
    "except TypeError:\n",
    "    ds_train = AirfRANS(root=scfg.root, train=True, transform=_PreparePhysics())\n",
    "    ds_test  = AirfRANS(root=scfg.root, train=False, transform=_PreparePhysics())\n",
    "\n",
    "if scfg.task == 'scarce':\n",
    "    # Scarce provides train only; create 90/10 split from ds_train\n",
    "    n = len(ds_train)\n",
    "    ids_all = list(range(n))\n",
    "    random.Random(scfg.seed).shuffle(ids_all)\n",
    "    ids_train = ids_all[:n]\n",
    "    # limit if requested\n",
    "    if scfg.limit_train > 0:\n",
    "        ids_train = ids_train[:scfg.limit_train + scfg.limit_val]\n",
    "        \n",
    "    train_raw = Subset(ds_train, ids_train)\n",
    "    val_raw = None\n",
    "    \n",
    "else:\n",
    "    ids_train = list(range(min(scfg.limit_train+scfg.limit_val, len(ds_train))))\n",
    "    ids_val = ids_train[-scfg.limit_val:] if scfg.limit_val>0 else []\n",
    "    ids_train = ids_train[:scfg.limit_train] if scfg.limit_train>0 else ids_train\n",
    "    train_raw = Subset(ds_train, ids_train)\n",
    "    val_raw   = Subset(ds_train, ids_val) if ids_val else []\n",
    "\n",
    "print('Loaded subset indices:', len(train_raw), 'train |', len(val_raw) if isinstance(val_raw, Subset) else 0, 'val/test')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b399587",
   "metadata": {},
   "source": [
    "# 4. Load prebuilt graphs and ensure features (index-aligned with raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "faa267d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[prebuilt] found: 200 train and 0 val graphs under prebuilt_edges/scarce\n",
      "Graphs prepared. Example dims -> x: torch.Size([16124, 5])  edge_attr: torch.Size([95510, 5])\n",
      "[validate] train_edges: total=200 bad=0\n"
     ]
    }
   ],
   "source": [
    "# 6) Load prebuilt graphs and ensure features (index-aligned with raw)\n",
    "import glob, os, re\n",
    "from utils import with_pos2, prep_graph, validate_edges, _prep_graph_for_norm\n",
    "\n",
    "USE_PREBUILT = True\n",
    "PREBUILT_ROOT = 'prebuilt_edges/scarce'  # change to your path if different\n",
    "PREBUILT_TRAIN_DIR = f\"{PREBUILT_ROOT}/train\"\n",
    "PREBUILT_TEST_DIR  = f\"{PREBUILT_ROOT}/test\"\n",
    "DOWNSAMPLED_ROOT = 'downsampled_graphs/scarce'\n",
    "\n",
    "# Load prebuilt edge graphs\n",
    "train_edge_files = sorted(glob.glob(os.path.join(PREBUILT_TRAIN_DIR, 'graph_*.pt')))\n",
    "val_edge_files   = sorted(glob.glob(os.path.join(PREBUILT_TEST_DIR,  'graph_*.pt')))\n",
    "print(f\"[prebuilt] found: {len(train_edge_files)} train and {len(val_edge_files)} val graphs under {PREBUILT_ROOT}\")\n",
    "\n",
    "# Load tensors and prepare\n",
    "train_edges = []\n",
    "for p in train_edge_files:\n",
    "    d = torch.load(p, map_location='cpu', weights_only=False)\n",
    "    if not isinstance(d, Data):\n",
    "        d = Data(**d)\n",
    "    train_edges.append(prep_graph(d))\n",
    "\n",
    "val_edges = []\n",
    "for p in val_edge_files:\n",
    "    d = torch.load(p, map_location='cpu', weights_only=False)\n",
    "    if not isinstance(d, Data):\n",
    "        d = Data(**d)\n",
    "    val_edges.append(prep_graph(d))\n",
    "\n",
    "print(f\"Graphs prepared. Example dims -> x: {train_edges[0].x.shape if len(train_edges)>0 else None}  edge_attr: {train_edges[0].edge_attr.shape if (len(train_edges)>0 and hasattr(train_edges[0],'edge_attr') and train_edges[0].edge_attr is not None) else None}\")\n",
    "\n",
    "validate_edges(train_edges, 'train_edges')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b54f62a4",
   "metadata": {},
   "source": [
    "# 5. Nnormalized datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7c83e76b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single sample BC check:\n",
      "  Total nodes: 16059\n",
      "  Original x shape: torch.Size([16059, 7])\n",
      "  Wall distance range: [0.000e+00, 3.563e+00]\n",
      "  Nodes with wall_dist < 1e-6: 1026\n",
      "  is_wall: 1026 nodes (6.4%)\n",
      "  is_inlet: 1645 nodes (10.2%)\n",
      "  is_outlet: 1833 nodes (11.4%)\n",
      "  is_farfield: 2289 nodes (14.3%)\n",
      "\n",
      "  Position ranges:\n",
      "    x: [-2.16, 4.23]\n",
      "    y: [-1.63, 1.62]\n",
      "Prepared normalized datasets: 180 train | 20 val\n",
      "Example dims -> x: (16059, 7) | edge_attr: (94686, 5)\n"
     ]
    }
   ],
   "source": [
    "if scfg.task == 'scarce':\n",
    "    n = len(train_edges)\n",
    "    n_train = int(n * 0.9)\n",
    "    ids_all = list(range(n))\n",
    "    random.Random(scfg.seed).shuffle(ids_all)\n",
    "    ids_train = ids_all[:n_train]\n",
    "    ids_val = ids_all[n_train:]\n",
    "\n",
    "    # Use prebuilt graphs, not raw dataset\n",
    "    train_edges_subset = [train_edges[i] for i in ids_train]\n",
    "    val_edges_subset = [train_edges[i] for i in ids_val] if ids_val else []\n",
    "else:\n",
    "    train_edges_subset = train_edges\n",
    "    val_edges_subset = val_edges\n",
    "\n",
    "train_prepped = [_prep_graph_for_norm(g) for g in train_edges_subset]\n",
    "val_prepped   = [_prep_graph_for_norm(g) for g in val_edges_subset] if isinstance(val_edges_subset, list) else []\n",
    "\n",
    "# 8b) Fit scalers on train_prepped\n",
    "if 'StandardScaler' not in globals():\n",
    "    class StandardScaler:\n",
    "        def __init__(self):\n",
    "            self.mean = None\n",
    "            self.std = None\n",
    "        def fit(self, t: torch.Tensor):\n",
    "            self.mean = t.mean(dim=0)\n",
    "            self.std = t.std(dim=0).clamp_min(1e-8)\n",
    "            return self\n",
    "        def transform(self, t: torch.Tensor):\n",
    "            return (t - self.mean) / self.std\n",
    "        def inverse(self, t: torch.Tensor):\n",
    "            return t * self.std + self.mean\n",
    "\n",
    "# Concatenate node features/targets across train graphs for fitting\n",
    "X_train = torch.cat([d.x for d in train_prepped if hasattr(d, 'x') and d.x is not None], dim=0)\n",
    "Y_train = torch.cat([d.y for d in train_prepped if hasattr(d, 'y') and d.y is not None], dim=0)\n",
    "\n",
    "x_scaler = StandardScaler().fit(X_train)\n",
    "y_scaler = StandardScaler().fit(Y_train)\n",
    "\n",
    "# 8c) Build normalized dataset wrappers\n",
    "class NormalizedDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, graphs, x_scaler, y_scaler):\n",
    "        self.graphs = graphs\n",
    "        self.x_scaler = x_scaler\n",
    "        self.y_scaler = y_scaler\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.graphs)\n",
    "        \n",
    "    def __getitem__(self, idx: int):\n",
    "        d = self.graphs[idx]\n",
    "        dm = Data(**{k: v for k, v in d})\n",
    "        dm.x = self.x_scaler.transform(d.x)\n",
    "        if hasattr(d, 'y') and d.y is not None:\n",
    "            dm.y = self.y_scaler.transform(d.y)\n",
    "        else:\n",
    "            dm.y = d.y\n",
    "            \n",
    "        # DON'T attach norm params as graph attributes - they cause batching issues\n",
    "        # Instead, we'll handle denormalization differently\n",
    "        # dm.x_norm_params = {'mean': self.x_scaler.mean.clone(), 'scale': self.x_scaler.std.clone()}\n",
    "        # dm.y_norm_params = {'mean': self.y_scaler.mean.clone(), 'scale': self.y_scaler.std.clone()} if dm.y is not None else None\n",
    "        \n",
    "        # Store scalers as module-level attributes for physics loss to access\n",
    "        dm.has_norm = True  # Flag to indicate normalized data\n",
    "        \n",
    "        # Ensure edge_attr_dxdy is present (needed for physics loss)\n",
    "        if hasattr(d, 'edge_attr_dxdy'):\n",
    "            dm.edge_attr_dxdy = d.edge_attr_dxdy\n",
    "        elif hasattr(d, 'edge_attr'):\n",
    "            # If we have edge_attr but not edge_attr_dxdy, use the last 2 dims as dxdy\n",
    "            if d.edge_attr.shape[1] >= 2:\n",
    "                dm.edge_attr_dxdy = d.edge_attr[:, -2:]  # Last 2 columns should be dx, dy\n",
    "            dm.edge_attr = d.edge_attr\n",
    "        \n",
    "        # Build BC masks properly\n",
    "        from airfrans_utils import build_bc_masks_airfrans\n",
    "        dm = build_bc_masks_airfrans(dm)\n",
    "        \n",
    "        # Ensure individual BC masks are present as attributes\n",
    "        if hasattr(dm, 'bc_mask_dict'):\n",
    "            for bc_type, mask in dm.bc_mask_dict.items():\n",
    "                setattr(dm, f'is_{bc_type}', mask)\n",
    "        else:\n",
    "            # Fallback: create default masks if build_bc_masks_airfrans failed\n",
    "            num_nodes = dm.x.size(0)\n",
    "            # Use the normalized x for BC detection\n",
    "            x_orig = d.x  # Use original (non-normalized) for BC detection\n",
    "            \n",
    "            # Wall nodes: distance_wall < threshold (column 2 of original x)\n",
    "            if x_orig.size(1) > 2:\n",
    "                wall_dist = x_orig[:, 2]\n",
    "                dm.is_wall = (wall_dist < 1e-6)\n",
    "            else:\n",
    "                dm.is_wall = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "            \n",
    "            # For AirfRANS, we typically don't have explicit inlet/outlet/farfield in the features\n",
    "            # These would need to be inferred from position or other criteria\n",
    "            dm.is_inlet = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "            dm.is_outlet = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "            dm.is_farfield = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "            \n",
    "            # Simple heuristics for inlet/outlet/farfield based on position\n",
    "            if hasattr(dm, 'pos'):\n",
    "                x_coords = dm.pos[:, 0]\n",
    "                y_coords = dm.pos[:, 1]\n",
    "                \n",
    "                # Inlet: leftmost boundary (x < -1)\n",
    "                dm.is_inlet = (x_coords < -1.0) & ~dm.is_wall\n",
    "                \n",
    "                # Outlet: rightmost boundary (x > 2)\n",
    "                dm.is_outlet = (x_coords > 2.0) & ~dm.is_wall\n",
    "                \n",
    "                # Farfield: top/bottom boundaries (|y| > 1)\n",
    "                dm.is_farfield = (torch.abs(y_coords) > 1.0) & ~dm.is_wall & ~dm.is_inlet & ~dm.is_outlet\n",
    "        \n",
    "        return dm\n",
    "\n",
    "train_norm = NormalizedDataset(train_prepped, x_scaler, y_scaler)\n",
    "val_norm   = NormalizedDataset(val_prepped, x_scaler, y_scaler) if isinstance(val_prepped, list) and len(val_prepped) > 0 else []\n",
    "\n",
    "# Debug BC mask creation for a single sample\n",
    "test_single = train_norm[0]\n",
    "print(\"Single sample BC check:\")\n",
    "print(f\"  Total nodes: {test_single.x.size(0)}\")\n",
    "\n",
    "# Check original features that determine BC\n",
    "if hasattr(test_single, 'x'):\n",
    "    x_orig = train_prepped[0].x  # Original unnormalized\n",
    "    print(f\"  Original x shape: {x_orig.shape}\")\n",
    "    if x_orig.size(1) > 2:\n",
    "        wall_dist = x_orig[:, 2]\n",
    "        print(f\"  Wall distance range: [{wall_dist.min():.3e}, {wall_dist.max():.3e}]\")\n",
    "        print(f\"  Nodes with wall_dist < 1e-6: {(wall_dist < 1e-6).sum().item()}\")\n",
    "\n",
    "# Check the BC masks\n",
    "for bc_type in ['wall', 'inlet', 'outlet', 'farfield']:\n",
    "    mask_name = f'is_{bc_type}'\n",
    "    if hasattr(test_single, mask_name):\n",
    "        mask = getattr(test_single, mask_name)\n",
    "        print(f\"  {mask_name}: {mask.sum().item()} nodes ({mask.sum().item()/len(mask)*100:.1f}%)\")\n",
    "\n",
    "# Check position-based criteria if available\n",
    "if hasattr(test_single, 'pos'):\n",
    "    pos = test_single.pos\n",
    "    print(f\"\\n  Position ranges:\")\n",
    "    print(f\"    x: [{pos[:, 0].min():.2f}, {pos[:, 0].max():.2f}]\")\n",
    "    print(f\"    y: [{pos[:, 1].min():.2f}, {pos[:, 1].max():.2f}]\")\n",
    "\n",
    "\n",
    "\n",
    "print('Prepared normalized datasets:', len(train_norm), 'train |', (len(val_norm) if isinstance(val_norm, NormalizedDataset) else len(val_norm)), 'val')\n",
    "if len(train_prepped) > 0:\n",
    "    print('Example dims -> x:', tuple(train_prepped[0].x.shape), '| edge_attr:', (tuple(train_prepped[0].edge_attr.shape) if hasattr(train_prepped[0], 'edge_attr') and train_prepped[0].edge_attr is not None else None))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c791019b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deg==0: 88  /  16059\n",
      "deg<2 : 88\n"
     ]
    }
   ],
   "source": [
    "data = train_norm[0]\n",
    "\n",
    "row, col = data.edge_index\n",
    "deg = torch.bincount(row, minlength=data.num_nodes) + torch.bincount(col, minlength=data.num_nodes)\n",
    "print(\"deg==0:\", int((deg==0).sum()), \" / \", data.num_nodes)\n",
    "print(\"deg<2 :\", int((deg<2).sum()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c374e06",
   "metadata": {},
   "source": [
    "# 6. DataLoaders\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "44753884",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaders ready: 180 train samples | batch_size = 19\n",
      "Loaders ready: 20 val samples | batch_size = 19\n"
     ]
    }
   ],
   "source": [
    "# Use true batching with PyG Batch.from_data_list so batch_size>1 works correctly\n",
    "\n",
    "def collate_pyg(batch):\n",
    "    batch = [b for b in batch if b is not None]\n",
    "    if len(batch) == 0:\n",
    "        return None\n",
    "    return Batch.from_data_list(batch)\n",
    "\n",
    "train_loader = DataLoader(train_norm, batch_size=scfg.batch_size, shuffle=True, num_workers=0, collate_fn=collate_pyg)\n",
    "val_loader   = DataLoader(val_norm,   batch_size=scfg.batch_size, shuffle=False, num_workers=0, collate_fn=collate_pyg) if isinstance(val_norm, NormalizedDataset) else []\n",
    "print('Loaders ready:', len(train_norm), 'train samples | batch_size =', scfg.batch_size)\n",
    "print('Loaders ready:', len(val_norm), 'val samples | batch_size =', scfg.batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "609e8624-66aa-426e-b01b-150d02207ef1",
   "metadata": {},
   "source": [
    "# Learning rate scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d99154cc-c1a7-4e2b-b31b-9a752b62092d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning Rate Scheduler 설정\n",
    "def create_lr_scheduler(optimizer, config):\n",
    "    \"\"\"Configuration에 따라 적절한 LR scheduler를 생성합니다.\"\"\"\n",
    "    \n",
    "    if config.lr_scheduler is None:\n",
    "        print(\"🚫 Learning rate scheduler: None (constant LR)\")\n",
    "        return None\n",
    "    \n",
    "    elif config.lr_scheduler == 'cosine':\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "            optimizer,\n",
    "            T_max=config.cosine_T_max,\n",
    "            eta_min=config.cosine_eta_min\n",
    "        )\n",
    "        print(f\"📊 Learning rate scheduler: CosineAnnealingLR\")\n",
    "        print(f\"   T_max: {config.cosine_T_max}, eta_min: {config.cosine_eta_min}\")\n",
    "        return scheduler\n",
    "    \n",
    "    elif config.lr_scheduler == 'cosine_warm_restarts':\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "            optimizer,\n",
    "            T_0=config.wr_T_0,\n",
    "            T_mult=config.wr_T_mult,\n",
    "            eta_min=config.wr_eta_min\n",
    "        )\n",
    "        print(f\"🔄 Learning rate scheduler: CosineAnnealingWarmRestarts\")\n",
    "        print(f\"   T_0: {config.wr_T_0}, T_mult: {config.wr_T_mult}, eta_min: {config.wr_eta_min}\")\n",
    "        return scheduler\n",
    "    \n",
    "    elif config.lr_scheduler == 'reduce_on_plateau':\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer,\n",
    "            mode='min',  # validation loss를 minimize\n",
    "            factor=config.rop_factor,\n",
    "            patience=config.rop_patience,\n",
    "            min_lr=config.rop_min_lr,\n",
    "        )\n",
    "        print(f\"📉 Learning rate scheduler: ReduceLROnPlateau\")\n",
    "        print(f\"   factor: {config.rop_factor}, patience: {config.rop_patience}, min_lr: {config.rop_min_lr}\")\n",
    "        return scheduler\n",
    "    \n",
    "    else:\n",
    "        print(f\"❌ Unknown scheduler: {config.lr_scheduler}, using None\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70110548-f5e7-47a1-bf66-00333cfeafdc",
   "metadata": {},
   "source": [
    "# Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fbc0d23d-fbd6-42e4-b76d-767d5c415273",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 13) Enhanced Train/Val epoch routines with Physics Loss\n",
    "\n",
    "mse_loss_fn = nn.MSELoss()\n",
    "\n",
    "def compute_loss_with_physics(predictions, targets, data, loss_fn=None, *, step: int | None = None):\n",
    "    \"\"\"Compute loss using physics-informed loss function or fallback to MSE\n",
    "    Returns a differentiable scalar loss tensor for backward as first value,\n",
    "    and a lightweight dict of float metrics for logging as second value.\n",
    "    \"\"\"\n",
    "    if loss_fn is not None:\n",
    "        try:\n",
    "            # Always let the physics loss handle batched Data (PyG batches are a big disjoint graph)\n",
    "            loss_dict = loss_fn(predictions, targets, data=data, step=step)\n",
    "\n",
    "            # Ensure total_loss is a Tensor usable for backward\n",
    "            total_loss = loss_dict.get('total_loss')\n",
    "            if not isinstance(total_loss, torch.Tensor):\n",
    "                total_loss = torch.as_tensor(total_loss, dtype=predictions.dtype, device=predictions.device)\n",
    "\n",
    "            # Prepare a logging-friendly dict (floats only) to avoid holding graph refs\n",
    "            log_dict = {}\n",
    "            for k, v in loss_dict.items():\n",
    "                if isinstance(v, torch.Tensor):\n",
    "                    try:\n",
    "                        log_dict[k] = float(v.detach().item())\n",
    "                    except Exception:\n",
    "                        # Fallback if it's not 0-dim\n",
    "                        log_dict[k] = float(v.detach().mean().item())\n",
    "                else:\n",
    "                    log_dict[k] = float(v)\n",
    "\n",
    "            return total_loss, log_dict\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Physics loss failed ({e}), falling back to MSE\")\n",
    "            mse_loss = mse_loss_fn(predictions, targets)\n",
    "            return mse_loss, {\n",
    "                'mse_loss': float(mse_loss.detach().item()), \n",
    "                'continuity_loss': 0.0, \n",
    "                'momentum_loss': 0.0,\n",
    "                'bc_loss': 0.0,  # ← BC loss 추가\n",
    "                'total_loss': float(mse_loss.detach().item())\n",
    "            }\n",
    "    else:\n",
    "        # Fallback to simple MSE\n",
    "        mse_loss = mse_loss_fn(predictions, targets)\n",
    "        return mse_loss, {\n",
    "            'mse_loss': float(mse_loss.detach().item()), \n",
    "            'bc_loss': 0.0,  # ← BC loss 추가\n",
    "            'total_loss': float(mse_loss.detach().item())\n",
    "        }\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def run_epoch(loader, model, device, scaler=None, desc: str = 'val', loss_fn=None):\n",
    "    model.eval()\n",
    "    total_losses = []; mse_losses = []; continuity_losses = []; momentum_losses = []\n",
    "    bc_losses = []  # ← BC loss 리스트 추가\n",
    "    cont_w_used_hist, mom_w_used_hist = [], []\n",
    "\n",
    "    if loader is None or (isinstance(loader, list) and len(loader)==0):\n",
    "        return float('nan'), {}\n",
    "\n",
    "    steps = len(loader)\n",
    "    pbar = tqdm(total=steps, desc=desc, leave=False)\n",
    "\n",
    "    for batch in loader:\n",
    "        try:\n",
    "            if batch is None:\n",
    "                pbar.update(1); continue\n",
    "\n",
    "            b = batch.to(device)\n",
    "            with (autocast(enabled=(scfg.amp and torch.cuda.is_available()))\n",
    "                  if torch.cuda.is_available() else contextlib.nullcontext()):\n",
    "                out = model(b)\n",
    "                _, loss_dict = compute_loss_with_physics(out, b.y, b, loss_fn=loss_fn, step=None)\n",
    "\n",
    "            total_losses.append(loss_dict['total_loss'])\n",
    "            mse_losses.append(loss_dict['mse_loss'])\n",
    "            continuity_losses.append(loss_dict.get('continuity_loss', 0.0))\n",
    "            momentum_losses.append(loss_dict.get('momentum_loss', 0.0))\n",
    "            bc_losses.append(loss_dict.get('bc_loss', 0.0))  # ← BC loss 수집\n",
    "            if 'cont_weight_used' in loss_dict: cont_w_used_hist.append(loss_dict['cont_weight_used'])\n",
    "            if 'mom_weight_used'  in loss_dict: mom_w_used_hist.append(loss_dict['mom_weight_used'])\n",
    "\n",
    "            postfix = {\"total\": f\"{loss_dict['total_loss']:.4e}\"}\n",
    "            if 'continuity_loss' in loss_dict: postfix[\"cont\"] = f\"{loss_dict['continuity_loss']:.4e}\"\n",
    "            if 'momentum_loss' in loss_dict:   postfix[\"momentum\"] = f\"{loss_dict['momentum_loss']:.4e}\"\n",
    "            if 'bc_loss' in loss_dict:         postfix[\"bc\"] = f\"{loss_dict['bc_loss']:.4e}\"  # ← BC loss 표시\n",
    "            pbar.set_postfix(postfix)\n",
    "\n",
    "        finally:\n",
    "            pbar.update(1)\n",
    "\n",
    "    pbar.close()\n",
    "\n",
    "    avg_losses = {\n",
    "        'total_loss': np.mean(total_losses) if total_losses else float('nan'),\n",
    "        'mse_loss': np.mean(mse_losses) if mse_losses else float('nan'),\n",
    "        'continuity_loss': np.mean(continuity_losses) if continuity_losses else float('nan'),\n",
    "        'momentum_loss': np.mean(momentum_losses) if momentum_losses else float('nan'),\n",
    "        'bc_loss': np.mean(bc_losses) if bc_losses else float('nan'),  # ← BC loss 평균\n",
    "    }\n",
    "    if cont_w_used_hist: avg_losses['cont_weight_used'] = float(np.mean(cont_w_used_hist))\n",
    "    if mom_w_used_hist:  avg_losses['mom_weight_used']  = float(np.mean(mom_w_used_hist))\n",
    "    return avg_losses['total_loss'], avg_losses\n",
    "\n",
    "\n",
    "\n",
    "def train_epoch(loader, model, optim, device, scaler, desc: str = 'train',\n",
    "                loss_fn=None, global_step_start: int = 0, scheduler=None, scheduler_step_mode: str = \"epoch\",\n",
    "                log_every_n_steps: int = -1):  # -1로 설정하면 step 로깅 비활성화\n",
    "    model.train()\n",
    "    total_losses, mse_losses, continuity_losses, momentum_losses = [], [], [], []\n",
    "    bc_losses = []\n",
    "    cont_w_used_hist, mom_w_used_hist = [], []\n",
    "\n",
    "    global_step = global_step_start\n",
    "    steps = len(loader)\n",
    "    pbar = tqdm(total=steps, desc=desc, leave=False)\n",
    "\n",
    "    for batch_idx, batch in enumerate(loader):\n",
    "        try:\n",
    "            if batch is None:\n",
    "                pbar.update(1); global_step += 1; continue\n",
    "\n",
    "            b = batch.to(device)\n",
    "            optim.zero_grad(set_to_none=True)\n",
    "\n",
    "            use_scaler = (scaler is not None) and getattr(scaler, \"is_enabled\", lambda: False)()\n",
    "\n",
    "            if use_scaler:\n",
    "                with autocast(enabled=torch.cuda.is_available()):\n",
    "                    out = model(b)\n",
    "                    loss, loss_dict = compute_loss_with_physics(out, b.y, b, loss_fn=loss_fn, step=global_step)\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.unscale_(optim)\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                scaler.step(optim)\n",
    "                scaler.update()\n",
    "            else:\n",
    "                with contextlib.nullcontext():\n",
    "                    out = model(b)\n",
    "                    loss, loss_dict = compute_loss_with_physics(out, b.y, b, loss_fn=loss_fn, step=global_step)\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                optim.step()\n",
    "\n",
    "            if scheduler is not None and scheduler_step_mode == \"step\":\n",
    "                try:\n",
    "                    scheduler.step()\n",
    "                except TypeError:\n",
    "                    pass\n",
    "\n",
    "            # 집계\n",
    "            total_losses.append(loss_dict['total_loss'])\n",
    "            mse_losses.append(loss_dict['mse_loss'])\n",
    "            continuity_losses.append(loss_dict.get('continuity_loss', 0.0))\n",
    "            momentum_losses.append(loss_dict.get('momentum_loss', 0.0))\n",
    "            bc_losses.append(loss_dict.get('bc_loss', 0.0))\n",
    "            if 'cont_weight_used' in loss_dict: cont_w_used_hist.append(loss_dict['cont_weight_used'])\n",
    "            if 'mom_weight_used'  in loss_dict: mom_w_used_hist.append(loss_dict['mom_weight_used'])\n",
    "\n",
    "            # === Step-level 로깅 완전 제거 또는 조건부 비활성화 ===\n",
    "            if log_every_n_steps > 0 and (batch_idx % max(1, log_every_n_steps)) == 0:\n",
    "                # Step-level 로깅을 원하는 경우에만 실행\n",
    "                log_payload = {\n",
    "                    \"step\": global_step,\n",
    "                    \"train/total\": loss_dict['total_loss'],\n",
    "                    \"train/mse\": loss_dict['mse_loss'],\n",
    "                    \"train/continuity\": loss_dict.get('continuity_loss', 0.0),\n",
    "                    \"train/momentum\": loss_dict.get('momentum_loss', 0.0),\n",
    "                    \"train/bc\": loss_dict.get('bc_loss', 0.0),\n",
    "                }\n",
    "                if 'cont_weight_used' in loss_dict: log_payload[\"weight/cont_used\"] = loss_dict['cont_weight_used']\n",
    "                if 'mom_weight_used'  in loss_dict: log_payload[\"weight/mom_used\"]  = loss_dict['mom_weight_used']\n",
    "                lr_now = get_lr(optim)\n",
    "                if lr_now is not None:\n",
    "                    log_payload[\"lr\"] = lr_now\n",
    "                wandb.log(log_payload, step=global_step, commit=False)\n",
    "\n",
    "            postfix = {\"total\": f\"{loss_dict['total_loss']:.4e}\",\n",
    "                       \"lr\": f\"{get_lr(optim):.2e}\" if get_lr(optim) is not None else \"n/a\"}\n",
    "            if 'continuity_loss' in loss_dict: postfix[\"cont\"] = f\"{loss_dict['continuity_loss']:.4e}\"\n",
    "            if 'momentum_loss' in loss_dict:   postfix[\"momentum\"] = f\"{loss_dict['momentum_loss']:.4e}\"\n",
    "            if 'bc_loss' in loss_dict:         postfix[\"bc\"] = f\"{loss_dict['bc_loss']:.4e}\"\n",
    "            pbar.set_postfix(postfix)\n",
    "\n",
    "        finally:\n",
    "            pbar.update(1)\n",
    "            global_step += 1\n",
    "\n",
    "    pbar.close()\n",
    "\n",
    "    avg_losses = {\n",
    "        'total_loss': np.mean(total_losses) if total_losses else float('nan'),\n",
    "        'mse_loss': np.mean(mse_losses) if mse_losses else float('nan'),\n",
    "        'continuity_loss': np.mean(continuity_losses) if continuity_losses else float('nan'),\n",
    "        'momentum_loss': np.mean(momentum_losses) if momentum_losses else float('nan'),\n",
    "        'bc_loss': np.mean(bc_losses) if bc_losses else float('nan'),\n",
    "    }\n",
    "    if cont_w_used_hist: avg_losses['cont_weight_used'] = float(np.mean(cont_w_used_hist))\n",
    "    if mom_w_used_hist:  avg_losses['mom_weight_used']  = float(np.mean(mom_w_used_hist))\n",
    "\n",
    "    return avg_losses['total_loss'], avg_losses, global_step"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f914514-b2af-4cb1-8458-e8f335d17c43",
   "metadata": {},
   "source": [
    "# Optuna configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a9d79032-8aaa-4e79-881b-8bdc4e142769",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Enhanced Global Context & Attention Mechanism loaded!\n",
      "Features: Multi-head attention, cross-attention, positional encoding, advanced pooling\n"
     ]
    }
   ],
   "source": [
    "# Add this cell after imports\n",
    "\n",
    "# Install optuna if not already installed\n",
    "# !pip install optuna optuna-dashboard\n",
    "\n",
    "import optuna\n",
    "from optuna.trial import TrialState\n",
    "from optuna.visualization import plot_optimization_history, plot_param_importances\n",
    "import joblib\n",
    "from global_context_processor import EnhancedCFDModelWithGlobalContext\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "    \"\"\"\n",
    "    Optuna objective function for hyperparameter optimization\n",
    "    Returns validation loss to minimize\n",
    "    \"\"\"\n",
    "\n",
    "    # 1) Model Architecture Hyperparameters\n",
    "    hidden_dim = trial.suggest_categorical('hidden_dim', [64, 128, 256, 512])\n",
    "    num_layers = trial.suggest_int('num_layers', 3, 10)\n",
    "    dropout_p = trial.suggest_float('dropout', 0.0, 0.5, step=0.05)\n",
    "\n",
    "    # 2) Training Hyperparameters\n",
    "    lr = trial.suggest_float('lr', 1e-5, 1e-2, log=True)\n",
    "    weight_decay = trial.suggest_float('weight_decay', 1e-6, 1e-1, log=True)\n",
    "    batch_size = trial.suggest_categorical('batch_size', [1, 2, 4, 8])\n",
    "\n",
    "    # 3) Optimizer Configuration\n",
    "    beta1 = trial.suggest_float('beta1', 0.8, 0.99)\n",
    "    beta2 = trial.suggest_float('beta2', 0.9, 0.999)\n",
    "    eps = trial.suggest_float('eps', 1e-9, 1e-6, log=True)\n",
    "\n",
    "    # 4) Physics Loss Hyperparameters\n",
    "    continuity_weight = trial.suggest_float('continuity_weight', 0.001, 0.5, log=True)\n",
    "    continuity_target_weight = trial.suggest_float(\n",
    "        'continuity_target_weight', continuity_weight, 1.0\n",
    "    )\n",
    "    momentum_weight = trial.suggest_float('momentum_weight', 0.001, 0.5, log=True)\n",
    "    momentum_target_weight = trial.suggest_float(\n",
    "        'momentum_target_weight', momentum_weight, 1.0\n",
    "    )\n",
    "    bc_loss_weight = trial.suggest_float('bc_loss_weight', 0.001, 0.2, log=True)\n",
    "\n",
    "    # 5) Curriculum Learning\n",
    "    ramp_start_epoch = trial.suggest_int('ramp_start_epoch', 5, 20)\n",
    "    ramp_epochs = trial.suggest_int('ramp_epochs', 10, 20)\n",
    "\n",
    "    # 6) Global Context (if using attention model)\n",
    "    use_global_tokens = trial.suggest_categorical('use_global_tokens', [True, False])\n",
    "    if use_global_tokens:\n",
    "        # suggest_int(name, low, high, step=None)도 가능하지만 여기선 카테고리로 명확히\n",
    "        num_global_tokens = trial.suggest_categorical('num_global_tokens', [2, 4, 8])\n",
    "        attention_heads = trial.suggest_categorical('attention_heads', [2, 4, 8])\n",
    "        attention_layers = trial.suggest_categorical('attention_layers', [2, 4, 8])\n",
    "        use_cross_attention = trial.suggest_categorical('use_cross_attention', [True, False])\n",
    "        positional_encoding = trial.suggest_categorical('positional_encoding', [True, False])\n",
    "        global_pooling_type = trial.suggest_categorical(\n",
    "            'global_pooling_type', ['mean', 'max', 'attention', 'set2set']\n",
    "        )\n",
    "    else:\n",
    "        # 사용 안 해도 config에 안전하게 채워 넣기 위한 기본값\n",
    "        num_global_tokens = 0\n",
    "        attention_heads = 4\n",
    "        attention_layers = 2\n",
    "        use_cross_attention = False\n",
    "        positional_encoding = False\n",
    "        global_pooling_type = 'mean'\n",
    "\n",
    "    # 7) Learning Rate Scheduler\n",
    "    lr_scheduler_type = trial.suggest_categorical(\n",
    "        'lr_scheduler', ['cosine', 'cosine_warm_restarts', 'reduce_on_plateau', None]\n",
    "    )\n",
    "\n",
    "    # ---- Create config with suggested parameters ----\n",
    "    config = SmokeCfg(\n",
    "        hidden=hidden_dim,\n",
    "        layers=num_layers,\n",
    "        lr=lr,\n",
    "        weight_decay=weight_decay,\n",
    "        batch_size=batch_size,\n",
    "        betas=(beta1, beta2),\n",
    "        eps=eps,\n",
    "        continuity_loss_weight=continuity_weight,\n",
    "        continuity_target_weight=continuity_target_weight,\n",
    "        momentum_loss_weight=momentum_weight,\n",
    "        momentum_target_weight=momentum_target_weight,\n",
    "        bc_loss_weight=bc_loss_weight,\n",
    "        ramp_start_epoch=ramp_start_epoch,\n",
    "        ramp_epochs=ramp_epochs,\n",
    "        use_global_tokens=use_global_tokens,\n",
    "        num_global_tokens=num_global_tokens,\n",
    "        attention_heads=attention_heads,\n",
    "        attention_layers=attention_layers,\n",
    "        use_cross_attention=use_cross_attention,\n",
    "        positional_encoding=positional_encoding,\n",
    "        global_pooling_type=global_pooling_type,\n",
    "        lr_scheduler=lr_scheduler_type,\n",
    "        epochs=20,              # Shorter for hyperparameter search\n",
    "        wandb_mode='disabled'   # Disable wandb during search\n",
    "    )\n",
    "\n",
    "    # ---- DataLoaders (batch size from trial) ----\n",
    "    train_loader_trial = DataLoader(\n",
    "        train_norm,\n",
    "        batch_size=config.batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=0,\n",
    "        collate_fn=collate_pyg\n",
    "    )\n",
    "    val_loader_trial = DataLoader(\n",
    "        val_norm,\n",
    "        batch_size=config.batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=0,\n",
    "        collate_fn=collate_pyg\n",
    "    )\n",
    "\n",
    "    # ---- Model ----\n",
    "    model_trial = EnhancedCFDModelWithGlobalContext(\n",
    "        node_feat_dim=7,\n",
    "        edge_feat_dim=5,\n",
    "        hidden_dim=config.hidden,\n",
    "        output_dim=4,\n",
    "        num_mp_layers=config.layers,\n",
    "        dropout_p=dropout_p,\n",
    "        config=config\n",
    "    ).to(device)\n",
    "\n",
    "    # ---- Optimizer ----\n",
    "    optimizer_trial = torch.optim.AdamW(\n",
    "        model_trial.parameters(),\n",
    "        lr=config.lr,\n",
    "        weight_decay=config.weight_decay,\n",
    "        betas=config.betas,\n",
    "        eps=config.eps\n",
    "    )\n",
    "\n",
    "    # ---- LR Scheduler ----\n",
    "    scheduler_trial = create_lr_scheduler(optimizer_trial, config)\n",
    "\n",
    "    # ---- Physics Loss ----\n",
    "    loss_fn_trial = NavierStokesPhysicsLoss(\n",
    "        data_loss_weight=getattr(config, 'data_loss_weight', 1.0),\n",
    "        continuity_loss_weight=config.continuity_loss_weight,\n",
    "        continuity_target_weight=config.continuity_target_weight,\n",
    "        momentum_loss_weight=config.momentum_loss_weight,\n",
    "        momentum_target_weight=config.momentum_target_weight,\n",
    "        curriculum_ramp_steps=config.ramp_epochs * max(1, len(train_loader_trial)),\n",
    "        ramp_start_step=config.ramp_start_epoch * max(1, len(train_loader_trial)),\n",
    "        bc_loss_weight=config.bc_loss_weight,\n",
    "        chord_length=getattr(config, 'chord_length', 1.0),\n",
    "        dynamic_uref_from_data=getattr(config, 'dynamic_uref_from_data', False),\n",
    "        dynamic_re_from_data=getattr(config, 'dynamic_re_from_data', False),\n",
    "        nu_molecular=getattr(config, 'nu_molecular', 1.5e-5),\n",
    "        use_huber_for_physics=getattr(config, 'use_huber_for_physics', False),\n",
    "        huber_delta=getattr(config, 'huber_delta', 1.0),\n",
    "        debug=False\n",
    "    )\n",
    "\n",
    "    # ---- Training loop ----\n",
    "    scaler = GradScaler(enabled=False)\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    max_patience = 5\n",
    "\n",
    "    for epoch in range(config.epochs):\n",
    "        # Train\n",
    "        model_trial.train()\n",
    "        global_step = epoch * max(1, len(train_loader_trial))\n",
    "\n",
    "        for batch in train_loader_trial:\n",
    "            if batch is None:\n",
    "                continue\n",
    "            batch = batch.to(device)\n",
    "            optimizer_trial.zero_grad()\n",
    "\n",
    "            predictions = model_trial(batch)\n",
    "            loss, _ = compute_loss_with_physics(\n",
    "                predictions, batch.y, batch,\n",
    "                loss_fn=loss_fn_trial, step=global_step\n",
    "            )\n",
    "\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model_trial.parameters(), 1.0)\n",
    "            optimizer_trial.step()\n",
    "            global_step += 1\n",
    "\n",
    "        # Validation\n",
    "        model_trial.eval()\n",
    "        val_losses = []\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader_trial:\n",
    "                if batch is None:\n",
    "                    continue\n",
    "                batch = batch.to(device)\n",
    "                predictions = model_trial(batch)\n",
    "                _, loss_dict = compute_loss_with_physics(\n",
    "                    predictions, batch.y, batch, loss_fn=loss_fn_trial\n",
    "                )\n",
    "                total = loss_dict.get('total_loss', None)\n",
    "                if total is None:\n",
    "                    # 혹시 키 이름이 다를 경우 대비\n",
    "                    total = loss_dict.get('loss', None)\n",
    "                if total is None:\n",
    "                    continue\n",
    "                # 텐서일 수 있으니 float로 변환\n",
    "                try:\n",
    "                    val_losses.append(float(total))\n",
    "                except Exception:\n",
    "                    val_losses.append(float(total.item()))\n",
    "\n",
    "        avg_val_loss = float(np.mean(val_losses)) if len(val_losses) > 0 else float('inf')\n",
    "\n",
    "        # Update scheduler\n",
    "        if scheduler_trial is not None:\n",
    "            if isinstance(scheduler_trial, torch.optim.lr_scheduler.ReduceLROnPlateau):\n",
    "                scheduler_trial.step(avg_val_loss)\n",
    "            else:\n",
    "                scheduler_trial.step()\n",
    "\n",
    "        # Early stopping\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= max_patience:\n",
    "                # print(f\"[EarlyStop] epoch={epoch} best_val={best_val_loss:.6f}\")\n",
    "                break\n",
    "\n",
    "        # Report to Optuna (for pruning)\n",
    "        trial.report(avg_val_loss, epoch)\n",
    "        if trial.should_prune():\n",
    "            raise optuna.exceptions.TrialPruned()\n",
    "\n",
    "    # Clean up\n",
    "    del model_trial\n",
    "    del optimizer_trial\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    return best_val_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "467a74c4-b8dd-485a-9034-d0e7a5a97f4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-20 23:07:21,205] A new study created in RDB with name: airfrans_gnn_optimization\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae1fe13338e74f05a21c600c01134425",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Learning rate scheduler: CosineAnnealingLR\n",
      "   T_max: 80, eta_min: 1e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kim\\AppData\\Local\\Temp\\ipykernel_7816\\2118916917.py:162: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[W 2025-09-20 23:07:56,636] Trial 0 failed with parameters: {'hidden_dim': 128, 'num_layers': 7, 'dropout': 0.1, 'lr': 0.0004, 'weight_decay': 0.01, 'batch_size': 2, 'beta1': 0.9, 'beta2': 0.95, 'eps': 1e-08, 'continuity_weight': 0.05, 'continuity_target_weight': 0.1, 'momentum_weight': 0.05, 'momentum_target_weight': 0.1, 'bc_loss_weight': 0.05, 'ramp_start_epoch': 10, 'ramp_epochs': 10, 'use_global_tokens': True, 'num_global_tokens': 4, 'attention_heads': 4, 'attention_layers': 4, 'use_cross_attention': True, 'positional_encoding': True, 'global_pooling_type': 'mean', 'lr_scheduler': 'cosine'} because of the following error: KeyboardInterrupt().\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Kim\\.conda\\envs\\pyg5090\\Lib\\site-packages\\optuna\\study\\_optimize.py\", line 201, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "                      ^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Kim\\AppData\\Local\\Temp\\ipykernel_7816\\2118916917.py\", line 184, in objective\n",
      "    loss.backward()\n",
      "  File \"c:\\Users\\Kim\\.conda\\envs\\pyg5090\\Lib\\site-packages\\torch\\_tensor.py\", line 647, in backward\n",
      "    torch.autograd.backward(\n",
      "  File \"c:\\Users\\Kim\\.conda\\envs\\pyg5090\\Lib\\site-packages\\torch\\autograd\\__init__.py\", line 354, in backward\n",
      "    _engine_run_backward(\n",
      "  File \"c:\\Users\\Kim\\.conda\\envs\\pyg5090\\Lib\\site-packages\\torch\\autograd\\graph.py\", line 829, in _engine_run_backward\n",
      "    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "[W 2025-09-20 23:07:56,639] Trial 0 failed with value None.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 47\u001b[39m\n\u001b[32m     45\u001b[39m \u001b[38;5;66;03m# Run optimization\u001b[39;00m\n\u001b[32m     46\u001b[39m n_trials = \u001b[32m100\u001b[39m  \u001b[38;5;66;03m# Number of trials to run\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m47\u001b[39m \u001b[43mstudy\u001b[49m\u001b[43m.\u001b[49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     48\u001b[39m \u001b[43m    \u001b[49m\u001b[43mobjective\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     49\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     50\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Can set timeout in seconds\u001b[39;49;00m\n\u001b[32m     51\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Use 1 for GPU, can increase for CPU-only\u001b[39;49;00m\n\u001b[32m     52\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     53\u001b[39m \u001b[43m    \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[32m     54\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     56\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m + \u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m*\u001b[32m50\u001b[39m)\n\u001b[32m     57\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mOptimization Complete!\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Kim\\.conda\\envs\\pyg5090\\Lib\\site-packages\\optuna\\study\\study.py:490\u001b[39m, in \u001b[36mStudy.optimize\u001b[39m\u001b[34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[39m\n\u001b[32m    388\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34moptimize\u001b[39m(\n\u001b[32m    389\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    390\u001b[39m     func: ObjectiveFuncType,\n\u001b[32m   (...)\u001b[39m\u001b[32m    397\u001b[39m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    398\u001b[39m ) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    399\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[32m    400\u001b[39m \n\u001b[32m    401\u001b[39m \u001b[33;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    488\u001b[39m \u001b[33;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[32m    489\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m490\u001b[39m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    491\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    492\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    493\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    494\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    495\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    496\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    497\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    498\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    499\u001b[39m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    500\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Kim\\.conda\\envs\\pyg5090\\Lib\\site-packages\\optuna\\study\\_optimize.py:63\u001b[39m, in \u001b[36m_optimize\u001b[39m\u001b[34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[39m\n\u001b[32m     61\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     62\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs == \u001b[32m1\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m63\u001b[39m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     64\u001b[39m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     65\u001b[39m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     66\u001b[39m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     67\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     68\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     69\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     70\u001b[39m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     71\u001b[39m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     72\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     73\u001b[39m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     74\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     75\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     76\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs == -\u001b[32m1\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Kim\\.conda\\envs\\pyg5090\\Lib\\site-packages\\optuna\\study\\_optimize.py:160\u001b[39m, in \u001b[36m_optimize_sequential\u001b[39m\u001b[34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[39m\n\u001b[32m    157\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m    159\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m160\u001b[39m     frozen_trial_id = \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    161\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    162\u001b[39m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[32m    163\u001b[39m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[32m    164\u001b[39m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[32m    165\u001b[39m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[32m    166\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Kim\\.conda\\envs\\pyg5090\\Lib\\site-packages\\optuna\\study\\_optimize.py:258\u001b[39m, in \u001b[36m_run_trial\u001b[39m\u001b[34m(study, func, catch)\u001b[39m\n\u001b[32m    251\u001b[39m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mShould not reach.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    253\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    254\u001b[39m     updated_state == TrialState.FAIL\n\u001b[32m    255\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    256\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[32m    257\u001b[39m ):\n\u001b[32m--> \u001b[39m\u001b[32m258\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[32m    259\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m trial._trial_id\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Kim\\.conda\\envs\\pyg5090\\Lib\\site-packages\\optuna\\study\\_optimize.py:201\u001b[39m, in \u001b[36m_run_trial\u001b[39m\u001b[34m(study, func, catch)\u001b[39m\n\u001b[32m    199\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial._trial_id, study._storage):\n\u001b[32m    200\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m201\u001b[39m         value_or_values = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    202\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions.TrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    203\u001b[39m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[32m    204\u001b[39m         state = TrialState.PRUNED\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 184\u001b[39m, in \u001b[36mobjective\u001b[39m\u001b[34m(trial)\u001b[39m\n\u001b[32m    178\u001b[39m predictions = model_trial(batch)\n\u001b[32m    179\u001b[39m loss, _ = compute_loss_with_physics(\n\u001b[32m    180\u001b[39m     predictions, batch.y, batch,\n\u001b[32m    181\u001b[39m     loss_fn=loss_fn_trial, step=global_step\n\u001b[32m    182\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m184\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    185\u001b[39m torch.nn.utils.clip_grad_norm_(model_trial.parameters(), \u001b[32m1.0\u001b[39m)\n\u001b[32m    186\u001b[39m optimizer_trial.step()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Kim\\.conda\\envs\\pyg5090\\Lib\\site-packages\\torch\\_tensor.py:647\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    637\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    638\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    639\u001b[39m         Tensor.backward,\n\u001b[32m    640\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    645\u001b[39m         inputs=inputs,\n\u001b[32m    646\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m647\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    648\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Kim\\.conda\\envs\\pyg5090\\Lib\\site-packages\\torch\\autograd\\__init__.py:354\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    349\u001b[39m     retain_graph = create_graph\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    353\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m354\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    362\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Kim\\.conda\\envs\\pyg5090\\Lib\\site-packages\\torch\\autograd\\graph.py:829\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    827\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    828\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m829\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    830\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    831\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    832\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    833\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Run the hyperparameter optimization\n",
    "\n",
    "# Create or load study\n",
    "study_name = \"airfrans_gnn_optimization\"\n",
    "storage_name = f\"sqlite:///{study_name}.db\"\n",
    "\n",
    "# Create study with pruning\n",
    "study = optuna.create_study(\n",
    "    study_name=study_name,\n",
    "    storage=storage_name,\n",
    "    direction='minimize',\n",
    "    pruner=optuna.pruners.MedianPruner(\n",
    "        n_startup_trials=5,\n",
    "        n_warmup_steps=5,\n",
    "        interval_steps=1\n",
    "    ),\n",
    "    sampler=optuna.samplers.TPESampler(seed=42),\n",
    "    load_if_exists=True\n",
    ")\n",
    "\n",
    "# Add default hyperparameters as the first trial (optional)\n",
    "study.enqueue_trial({\n",
    "    'hidden_dim': 128,\n",
    "    'num_layers': 7,\n",
    "    'dropout': 0.1,\n",
    "    'lr': 4e-4,\n",
    "    'weight_decay': 1e-2,\n",
    "    'batch_size': 2,\n",
    "    'beta1': 0.9,\n",
    "    'beta2': 0.95,\n",
    "    'eps': 1e-8,\n",
    "    'continuity_weight': 0.05,\n",
    "    'continuity_target_weight': 0.10,\n",
    "    'momentum_weight': 0.05,\n",
    "    'momentum_target_weight': 0.10,\n",
    "    'bc_loss_weight': 0.05,\n",
    "    'ramp_start_epoch': 10,\n",
    "    'ramp_epochs': 10,\n",
    "    'use_global_tokens': True,\n",
    "    'num_global_tokens': 4,\n",
    "    'attention_heads': 4,\n",
    "    'lr_scheduler': 'cosine'\n",
    "})\n",
    "\n",
    "# Run optimization\n",
    "n_trials = 100  # Number of trials to run\n",
    "study.optimize(\n",
    "    objective, \n",
    "    n_trials=n_trials,\n",
    "    timeout=None,  # Can set timeout in seconds\n",
    "    n_jobs=1,  # Use 1 for GPU, can increase for CPU-only\n",
    "    gc_after_trial=True,\n",
    "    show_progress_bar=True\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Optimization Complete!\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Number of finished trials: {len(study.trials)}\")\n",
    "print(f\"Best trial value: {study.best_value:.6f}\")\n",
    "print(\"\\nBest parameters:\")\n",
    "for key, value in study.best_params.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "236674a4-aef2-4944-843f-2a3996fae734",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze and visualize results\n",
    "\n",
    "# 1. Optimization History\n",
    "fig = optuna.visualization.plot_optimization_history(study)\n",
    "fig.show()\n",
    "\n",
    "# 2. Parameter Importance\n",
    "fig = optuna.visualization.plot_param_importances(study)\n",
    "fig.show()\n",
    "\n",
    "# 3. Parallel Coordinate Plot\n",
    "fig = optuna.visualization.plot_parallel_coordinate(\n",
    "    study, \n",
    "    params=['hidden_dim', 'num_layers', 'lr', 'continuity_weight', 'momentum_weight']\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "# 4. Slice Plot for specific parameters\n",
    "fig = optuna.visualization.plot_slice(\n",
    "    study,\n",
    "    params=['lr', 'hidden_dim', 'num_layers', 'batch_size']\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "# 5. Get statistics\n",
    "completed_trials = [t for t in study.trials if t.state == TrialState.COMPLETE]\n",
    "pruned_trials = [t for t in study.trials if t.state == TrialState.PRUNED]\n",
    "\n",
    "print(f\"Statistics:\")\n",
    "print(f\"  Completed trials: {len(completed_trials)}\")\n",
    "print(f\"  Pruned trials: {len(pruned_trials)}\")\n",
    "print(f\"  Best trial: #{study.best_trial.number}\")\n",
    "print(f\"  Best value: {study.best_value:.6f}\")\n",
    "\n",
    "# 6. Top 5 trials\n",
    "df = study.trials_dataframe()\n",
    "df_sorted = df.sort_values('value').head(5)\n",
    "print(\"\\nTop 5 trials:\")\n",
    "print(df_sorted[['number', 'value', 'params_hidden_dim', 'params_lr', 'params_num_layers']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe7d55b-2c6e-41ee-86d4-2902fbcf90ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train final model with best hyperparameters\n",
    "\n",
    "def train_with_best_params(study, epochs=100):\n",
    "    \"\"\"Train model with the best hyperparameters found\"\"\"\n",
    "    \n",
    "    best_params = study.best_params\n",
    "    print(f\"Training with best parameters from trial #{study.best_trial.number}\")\n",
    "    \n",
    "    # Update configuration with best parameters\n",
    "    final_config = SmokeCfg(\n",
    "        hidden=best_params['hidden_dim'],\n",
    "        layers=best_params['num_layers'],\n",
    "        lr=best_params['lr'],\n",
    "        weight_decay=best_params['weight_decay'],\n",
    "        batch_size=best_params['batch_size'],\n",
    "        betas=(best_params['beta1'], best_params['beta2']),\n",
    "        eps=best_params['eps'],\n",
    "        continuity_loss_weight=best_params['continuity_weight'],\n",
    "        continuity_target_weight=best_params['continuity_target_weight'],\n",
    "        momentum_loss_weight=best_params['momentum_weight'],\n",
    "        momentum_target_weight=best_params['momentum_target_weight'],\n",
    "        bc_loss_weight=best_params['bc_loss_weight'],\n",
    "        ramp_start_epoch=best_params['ramp_start_epoch'],\n",
    "        ramp_epochs=best_params['ramp_epochs'],\n",
    "        use_global_tokens=best_params['use_global_tokens'],\n",
    "        num_global_tokens=best_params.get('num_global_tokens', 4),\n",
    "        attention_heads=best_params.get('attention_heads', 4),\n",
    "        lr_scheduler=best_params['lr_scheduler'],\n",
    "        epochs=epochs,\n",
    "        wandb_mode='online'  # Enable wandb for final training\n",
    "    )\n",
    "    \n",
    "    # Create new model and train\n",
    "    # ... (use your existing training code with final_config)\n",
    "    \n",
    "    return final_config\n",
    "\n",
    "# Train final model\n",
    "final_config = train_with_best_params(study, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5337df87-d400-4793-bf40-872f0aeecac3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyg5090",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
